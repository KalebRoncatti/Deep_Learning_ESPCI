{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.autograd as ag\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import gzip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to set up a simple classifier for text and sentiment analysis. \n",
    "\n",
    "The goal of this lab session is to implement the model proposed by  Yoon Kim, published in 2014. This model is a sentence classifier based on Convolution. The original paper can be found [here](https://www.aclweb.org/anthology/D14-1181). It was then adapted to DNA sequence classification by [this paper](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1878-3). Of course, there exists pytorch and tensorflow implementations on the web. They are more or less correct and efficient. However, here it is important to do it yourself. The goal is to better understand pytorch and the convolution. \n",
    "\n",
    "The task is the binary classification of movie reviews. The dataset is a part of the imdb dataset. You can find the original dataset on the imdb website or a version on the kaggle website. For this lab session, we will use a preprocessed version and reduced version. \n",
    "\n",
    "The road-map is to: \n",
    "- Load the data\n",
    "- step-by-step computation (debug)\n",
    "- Create a model to wrap the convolution and pooling \n",
    "\n",
    "\n",
    "\n",
    "# Data loading \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'torch.Tensor'> <class 'dict'>\n",
      "tensor([ 36,  25, 381,  10,  58,  21,  83])\n",
      "nb examples :  30000\n",
      "Vocab size:  5002\n"
     ]
    }
   ],
   "source": [
    "fp = gzip.open('imdb.pck.gz','rb')\n",
    "texts , labels, lexicon  = pickle.load(fp) \n",
    "\n",
    "print(type(texts), type(labels), type(lexicon))\n",
    "print(texts[0])\n",
    "print(\"nb examples : \", len(texts))\n",
    "VOCAB_SIZE = len(lexicon)\n",
    "print(\"Vocab size: \", VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You get 3 objects : \n",
    "- *texts*  : a list of tensors, each tensor represent a word sequence to classify. \n",
    "- *labels* : the class, positive or negative, of the corresponding text\n",
    "- *lexicon*: a dictionnary to map integers to real words\n",
    "Note that a reduced number of words are selected to build the vocabulary. The less frequent words are discarded are replaced by a specific form (*unk* for unknown)\n",
    "To read the text you can use for example the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7])\n",
      "['strong', 'drama']\n",
      "['please', 'remake', 'this', 'movie']\n",
      "['very', 'funny', '!']\n",
      "['great', 'series']\n",
      "['fun', 'movie']\n",
      "------------\n",
      "['absolute', 'waste', 'of', 'time']\n",
      "['the', 'worst', 'movie', 'ever', 'made']\n",
      "['slow', 'motion', 'picture', 'that', 'did', \"n't\", 'get', 'to', 'the', 'point']\n",
      "['there', 'are', 'good', 'bad', 'movies', 'and', 'there', 'are', 'bad', 'bad', 'movies', 'this', 'one', 'is', 'a', 'real', 'stinker']\n",
      "['<unk>', 'so', 'bad', 'its', 'funny']\n"
     ]
    }
   ],
   "source": [
    "def idx2wordlist(idx_array,lexicon): \n",
    "    l = []\n",
    "    for i in idx_array: \n",
    "        l.append(lexicon[i.item()])\n",
    "    return l\n",
    "print(texts[0].shape)\n",
    "for i in range(5): \n",
    "    print(idx2wordlist(texts[i+50],lexicon))\n",
    "print(\"------------\")\n",
    "for i in range(5): \n",
    "    print(idx2wordlist(texts[-i-2000],lexicon))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and Convolution layers\n",
    "\n",
    "Unfortunately, an important part of the work is dedicated to playing with dimensions. This is true for pytorch, as well as tensorflow. Here the sequence of operation is \n",
    "- Embedding\n",
    "- Convolution (1D)\n",
    "- Pooling\n",
    "- Linear\n",
    "\n",
    "Moreover, things can be tricky if we want our model to work properly with mini-batch. \n",
    "\n",
    "\n",
    "A quick reminder on Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = 4 # dimension of embeddings, the input size for convolution\n",
    "h2 = 2 # output dimension (filter size) for the convolution\n",
    "embLayer = th.nn.Embedding(num_embeddings=len(lexicon), embedding_dim=h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the sequence :  3\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Don't play with the first sentence, it's only one word ! \n",
    "embs = embLayer(texts[1])\n",
    "print(\"the length of the sequence : \", len(texts[1]))\n",
    "print(embs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Look at the documentation of the Conv1d layer. Read it carefully and try to completely understand the following code. A convolution layer expects a tensor as input, with the following dimensions *B,D,L*: \n",
    "- B: size of the batch, the number of examples (here the number of sequence). For the moment we consider *B=1* (only one sequence)\n",
    "- D: the dimension of the vectors for each time step\n",
    "- L: the length of the input sequence (the number of time step)\n",
    "\n",
    "We must therefore modify the dimensions of the tensor generated by the embedding layer accordingly. \n",
    "\n",
    "A first solution could be: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "tmp = embs.view(1,h1,-1)\n",
    "print(tmp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape is correct, but it is safer to check the consistency: the first time step should be the embedding of the first word of the sequence. Is that correct ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5888, 1.1686, 0.6045, 0.3685], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "#### TODO : \n",
    "print(tmp[0,:,0]) # the embedding of the first time step\n",
    "#### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the good way to tranform embs in consistent way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO \n",
    "tmp = None ## <--Find the right way\n",
    "print(tmp.shape)\n",
    "print(tmp[0,:,0])\n",
    "## while  the expected value is : \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a tensor to feed the convolution layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embs :  torch.Size([3, 4])\n",
      "tmp  :  torch.Size([1, 4, 3])\n",
      "conv :  torch.Size([1, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "conv1 = th.nn.Conv1d(in_channels=4,out_channels=2,kernel_size=3)\n",
    "\n",
    "res = conv1(tmp)\n",
    "print(\"embs : \",embs.shape)\n",
    "print(\"tmp  : \",tmp.shape)\n",
    "print(\"conv : \",res.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw what happens to better understand the obtained dimensions. \n",
    "\n",
    "Now if we add another parameter for padding (set to 1). What do you observe ? \n",
    "Play a bit with the *kernel_size* along with the *padding* to understand the interaction: \n",
    "- try kernel_size=3,padding=1 and (4,1)\n",
    "- (5,1) and (5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1 = th.nn.Conv1d(in_channels=h1,out_channels=h2,kernel_size=3,padding=1)\n",
    "tmp=embs.view(1,4,-1)\n",
    "res = conv1(tmp)\n",
    "print(embs.shape)\n",
    "print(tmp.shape)\n",
    "print(res.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you propose for pooling ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first model\n",
    "\n",
    "First we want to create a model with an embedding layer of size 20, followed by a convolution layer: \n",
    "- feature size of 10, \n",
    "- kernel size of 3 (for trigram),\n",
    "- padding set to 1 \n",
    "\n",
    "All these values must be  parameters.\n",
    "\n",
    "\n",
    "## Interlude: object programming \n",
    "\n",
    "To write our own module, we need to write a class that inherits from Module. A class is a kind of data structure associated to function called methods to manipulate the data. A class allows you to build object, instance of the class. A class is a kind of special type with specific tools (methods) to handle instance of this type. In the definition of a class,  the keyword **self** is by convention a reference to the running object. \n",
    "\n",
    "Below is a simple example of a class, just for illustration purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A point : (4,2)\n",
      "A point : (0,-1)\n",
      "5.0  ==  5.0\n",
      "A point : (0,0)\n"
     ]
    }
   ],
   "source": [
    "# Defining a dummy class for a point in 2D\n",
    "class Point2D: \n",
    "    # The constructor is the method used to create an object of this class. \n",
    "    # It ensures that the object (the structure) is properly created, \n",
    "    # with everything we need. A constructor does not return anything. \n",
    "    def __init__(self, a=0 , b=0): \n",
    "        self.abs = a # defines an attribute for absciss\n",
    "        self.ord = b # defines an attribute for ordinate\n",
    "        # The default value of absciss and ordinate are 0 in this case\n",
    "    # The method called by print\n",
    "    # It returns a string\n",
    "    def __str__(self): \n",
    "        return \"A point : (\"+str(self.abs)+\",\"+str(self.ord)+\")\"\n",
    "    \n",
    "    \n",
    "    # A regular method : the  distance with an another point\n",
    "    def distance(self, p2): \n",
    "        return math.sqrt((self.abs-p2.abs)**2 + (self.ord-p2.ord)**2)\n",
    "    \n",
    "p1 = Point2D(4,2)\n",
    "print(p1)\n",
    "p2 = Point2D(b=-1)\n",
    "print(p2)\n",
    "print(p1.distance(p2), \" == \",p2.distance(p1))\n",
    "p3 = Point2D()\n",
    "print(p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A class for our model\n",
    "\n",
    "The goal now is to write a class to implement the model with embeddings, convolution and pooling. Writing this class, allows you to wrap what you have seen so far. To debug the model, you can first play step-by-step with each layer to ensure you are right with dimensions (it was done earlier). Then, write the class and run the training to evaluate the result (this what we have to do now). \n",
    "\n",
    "The class inherits from an existing class of pytorch : *Module*. This means that *Conv1d_classifier* is a *Module*, but we add some peculiarities. For that purpose we \n",
    "can fill the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conv1d_classifier(nn.Module):\n",
    "    '''A text classifier:\n",
    "    - input = a list of word indices\n",
    "    - output = probability associated to a binary classification task\n",
    "    - vocab_size: the number of words in the vocabulary we want to embed\n",
    "    - embedding_dim: size of the word vectors\n",
    "    '''\n",
    "    def __init__(self, vocab_size, embedding_dim, feat_size=10, kernel_size=3,lmax=35):\n",
    "        super(Conv1d_classifier, self).__init__()\n",
    "        self.emb_dim = embedding_dim \n",
    "        # in the previous line, \n",
    "        # store the value of the parameter embedding_dim\n",
    "        # TODO : write the end of the constructor\n",
    "        # It is important to create here all the layers of the network. \n",
    "        # All layers that have paramaters should be attribute. \n",
    "        # For example: \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Go on and add the rest. \n",
    "        # TODO ... \n",
    "        \n",
    "        \n",
    "            \n",
    "    def forward(self, input):\n",
    "        # TODO\n",
    "        # if you need to run forward with the embedding layer, \n",
    "        # you can call it by self.embeddings \n",
    "        # TODO ... \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the class: is everything in place:\n",
    "# A first classifier is built like : \n",
    "classifier = Conv1d_classifier(vocab_size=VOCAB_SIZE,embedding_dim=10)\n",
    "# The parameters of the classifier are randomly initialize, but we \n",
    "# can use it on a sequence : \n",
    "out = classif.forward(texts[0])\n",
    "print(out.shape) # the output has 2 dimensions \n",
    "print(out)\n",
    "\n",
    "# It is correct ? If not, correct the class to get the expected result. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "To train the model, we need to define a loss function and an optimizer. For the moment we will rely on an online learning algorithm: online stochastic gradient descent. Like the previous lab session: \n",
    "- we pick one training example\n",
    "- compute the loss\n",
    "- back-propagation of the gradient \n",
    "- update of the parameters\n",
    "\n",
    "\n",
    "At the end of one epoch, we evaluate the model on the validation step. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the training loss \n",
    "loss_function = nn.BCELoss()\n",
    "# The optimizer \n",
    "optimizer = th.optim.Adam(classif1.parameters(), lr=0.01)\n",
    "# Handle the randomization of the training data \n",
    "total = len(texts)\n",
    "ntrain = 20000  # the number of texts for training \n",
    "assert(total > ntrain) # be sure it is correct\n",
    "## \n",
    "randomidx = list(range(total))\n",
    "random.shuffle(randomidx)\n",
    "## random selection of training examples \n",
    "trainidx  = randomidx[:ntrain]\n",
    "## and for validation \n",
    "valididx  = randomidx[ntrain:]\n",
    "## \n",
    "Nepoch = 10 # the number of training epochs \n",
    "for e in range(Nepoch): \n",
    "    # randomized the training set \n",
    "    random.shuffle(trainidx)\n",
    "    for i in trainidx:\n",
    "        # TODO : training\n",
    "        # \n",
    "    ## validation score \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State of the art model\n",
    "\n",
    "To have a better model, we should add convolution layers of different kernel size, as in the paper of Yoon Kim 2014. \n",
    "We can use kernels of size 3,5, and 7 for instance. Create a new class for this model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finaly add dropout on the last layer hidden layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch training\n",
    "\n",
    "It is really faster to train the model with mini-batch. The issue is that the input sequences are not of the same size. As a workaround, we can write a function that create the tensor  for a mini-batch. This function needs: \n",
    "- a reference of the data (here texts)\n",
    "- the maximum length of a sequence in the mini-batch\n",
    "- a list of the indices of the sequences we want to put in the mini-batch\n",
    "\n",
    "The function creates a tensor and fill it with the selected sequences, but : \n",
    "- if a sequence is shorter than the maximum length, we pad the sequence with zero values (fill the empty slots)\n",
    "- if the sequence is longer, just truncate it. \n",
    "This function returns a tensor of dimensions (B,Lmax) to be the input of the embedding layer of our model. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
